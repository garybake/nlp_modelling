{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Using Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "We'll use a dataset of news articles grouped into 20 news categories - but just use 7 for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'comp.windows.x',\n",
    "    'rec.autos',\n",
    "    'rec.sport.baseball',\n",
    "    'rec.sport.hockey',\n",
    "    'sci.space',\n",
    "    'soc.religion.christian',\n",
    "    'talk.politics.guns'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.windows.x\n",
      "rec.autos\n",
      "rec.sport.baseball\n",
      "rec.sport.hockey\n",
      "sci.space\n",
      "soc.religion.christian\n",
      "talk.politics.guns\n"
     ]
    }
   ],
   "source": [
    "for newsgroup in newsgroups_train.target_names:\n",
    "    print(newsgroup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some distinct themes in the news categories like sports, religion, science, technology, politics etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From: rlennip4@mach1.wlu.ca (robert lennips 9209 U)\\nSubject: Re: PLANETS STILL: IMAGES ORBIT BY ETHER TWIST\\nX-Newsreader: TIN [version 1.1 PL6]\\nOrganization: Wilfrid Laurier University\\nLines: 2\\n\\nPlease get a REAL life.\\n\\n',\n",
       " \"From: rdetweil@boi.hp.com (Richard Detweiler)\\nSubject: Cards Mailing List?\\nDistribution: usa\\nOrganization: Hewlett Packard\\nLines: 9\\n\\nCount me interested in a Cardinal's mailing list.  If anyone\\nfinds one or starts one, please let me know.\\n\\nThanks,\\n\\nDick Detweiler\\n\\nrdetweil@hpdmd48.boi.hp.com\\n\\n\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4122,) (4122,)\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.filenames.shape, newsgroups_train.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We will perform the following steps:\n",
    "\n",
    "* **Tokenization**: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "* Words that have fewer than 3 characters are removed.\n",
    "* All **stopwords** are removed.\n",
    "* Words are **lemmatized** - words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "* Words are **stemmed** - words are reduced to their root form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')  # Porter2 stemmer\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    lemmatized = WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "    return stemmer.stem(lemmatized)\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Tokenise and lemmatize text\n",
    "    \"\"\"\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['This', 'disk', 'has', 'failed', 'many', 'times.', 'I', 'would', 'like', 'to', 'get', 'it', 'replaced.']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['disk', 'fail', 'time', 'like', 'replac']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = 'This disk has failed many times. I would like to get it replaced.'\n",
    "\n",
    "print(\"Original document: \")\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess all the messages we have (in parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "pool = multiprocessing.Pool()\n",
    "processed_docs = list(pool.map(preprocess, newsgroups_train.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['rlennip', 'mach', 'robert', 'lennip', 'subject', 'planet', 'imag', 'orbit', 'ether', 'twist', 'newsread', 'version', 'organ', 'wilfrid', 'laurier', 'univers', 'line', 'real', 'life'], ['rdetweil', 'richard', 'detweil', 'subject', 'card', 'mail', 'list', 'distribut', 'organ', 'hewlett', 'packard', 'line', 'count', 'interest', 'cardin', 'mail', 'list', 'find', 'start', 'know', 'thank', 'dick', 'detweil', 'rdetweil', 'hpdmd']]\n"
     ]
    }
   ],
   "source": [
    "print(processed_docs[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bag of words\n",
    "\n",
    "A dictionary is the number of times a word appears in the training set.\n",
    "A mapping between words and their integer ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ether\n",
      "1 imag\n",
      "2 laurier\n",
      "3 lennip\n",
      "4 life\n",
      "5 line\n",
      "6 mach\n"
     ]
    }
   ],
   "source": [
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    if k > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out tokens that appear in\n",
    "* less than 15 documents or\n",
    "* more than 10% of documents\n",
    "* after (1) and (2), keep only the first 100k most frequent tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert document (a list of words) into the bag-of-words format.  \n",
    "A list of (token_id, token_count) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(277, 1) - devic\n"
     ]
    }
   ],
   "source": [
    "bow_doc_x = bow_corpus[10]\n",
    "bow_word_x = 3\n",
    "\n",
    "print('{} - {}'.format(\n",
    "    bow_doc_x[5],\n",
    "    dictionary[bow_doc_x[bow_word_x][0]]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the LDA Model\n",
    "(Latent Dirichlet Allocation)  \n",
    "If observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **alpha** and **eta** are hyperparameters that affect sparsity of the document-topic (theta) and topic-word (lambda) distributions. We will let these be the default values for now(default value is `1/num_topics`)\n",
    "    - Alpha is the per document topic distribution.\n",
    "        * High alpha: Every document has a mixture of all topics(documents appear similar to each other).\n",
    "        * Low alpha: Every document has a mixture of very few topics\n",
    "\n",
    "    - Eta is the per topic word distribution.\n",
    "        * High eta: Each topic has a mixture of most words(topics appear similar to each other).\n",
    "        * Low eta: Each topic has a mixture of few words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(\n",
    "    bow_corpus,\n",
    "    num_topics=7,\n",
    "    id2word=dictionary,                                    \n",
    "    passes=10,\n",
    "    workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.019*\"christian\" + 0.008*\"exist\" + 0.007*\"truth\" + 0.005*\"live\" + 0.005*\"life\" + 0.005*\"claim\" + 0.005*\"religion\" + 0.005*\"belief\" + 0.004*\"true\" + 0.004*\"absolut\"'),\n",
       " (1,\n",
       "  '0.010*\"player\" + 0.007*\"season\" + 0.006*\"hockey\" + 0.006*\"score\" + 0.004*\"leagu\" + 0.004*\"goal\" + 0.004*\"basebal\" + 0.004*\"playoff\" + 0.004*\"defens\" + 0.004*\"second\"'),\n",
       " (2,\n",
       "  '0.014*\"jesus\" + 0.012*\"church\" + 0.007*\"christ\" + 0.006*\"bibl\" + 0.006*\"christian\" + 0.006*\"hell\" + 0.006*\"faith\" + 0.005*\"cathol\" + 0.005*\"paul\" + 0.005*\"father\"'),\n",
       " (3,\n",
       "  '0.023*\"window\" + 0.011*\"server\" + 0.011*\"widget\" + 0.010*\"file\" + 0.010*\"program\" + 0.009*\"motif\" + 0.008*\"applic\" + 0.008*\"display\" + 0.008*\"avail\" + 0.007*\"version\"'),\n",
       " (4,\n",
       "  '0.013*\"file\" + 0.009*\"entri\" + 0.009*\"weapon\" + 0.008*\"gun\" + 0.008*\"firearm\" + 0.006*\"control\" + 0.005*\"crime\" + 0.005*\"govern\" + 0.005*\"output\" + 0.005*\"program\"'),\n",
       " (5,\n",
       "  '0.024*\"space\" + 0.014*\"nasa\" + 0.010*\"orbit\" + 0.009*\"launch\" + 0.006*\"satellit\" + 0.005*\"mission\" + 0.005*\"earth\" + 0.005*\"data\" + 0.004*\"moon\" + 0.004*\"henri\"'),\n",
       " (6,\n",
       "  '0.008*\"engin\" + 0.007*\"car\" + 0.005*\"price\" + 0.005*\"batf\" + 0.005*\"dealer\" + 0.005*\"drive\" + 0.004*\"compound\" + 0.004*\"scott\" + 0.004*\"buy\" + 0.003*\"children\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_map = {\n",
    "    3: 'comp.windows.x',\n",
    "    6: 'rec.autos',\n",
    "    -1: 'rec.sport.baseball',\n",
    "    1: 'rec.sport.hockey',\n",
    "    5: 'sci.space',\n",
    "    0: 'soc.religion.christian',\n",
    "    2: 'soc.religion.christian',\n",
    "    4: 'talk.politics.guns'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: eggertj@moses.ll.mit.edu (Jim Eggert x6127 g41)\n",
      "Subject: Re: Robin Lane Fox's _The Unauthorized Version_?\n",
      "Reply-To: eggertj@ll.mit.edu\n",
      "Organization: MIT Lincoln Lab - Group 41\n",
      "Lines: 19\n",
      "\n",
      "In article <May.7.01.09.39.1993.14550@athos.rutgers.edu> iscleekk@nuscc.nus.sg (LEE KOK KIONG JAMES) writes:\n",
      "|   mpaul@unl.edu (marxhausen paul) writes:\n",
      "|   > My mom passed along a lengthy review she clipped regarding Robin Lane\n",
      "|   > Fox's book _The Unauthorized Version: Truth and Fiction in the Bible_,\n",
      "|...\n",
      "|   I've read the book. Some parts were quite typical regarding its\n",
      "|   criticism of the bible as an inaccurate historical document,\n",
      "|   alt.altheism, etc carries typical responses, but not as vociferous as\n",
      "|   a.a. It does give an insight into how these historian (is he one... I \n",
      "|   don't have any biodata on him) work. I've not been able to understand/\n",
      "|   appreciate some of the arguments, something like, it mentions certain \n",
      "|   events, so it has to be after that event, and so on. \n",
      "\n",
      "Robin Lane Fox is a historian and a gardener.  He has written several\n",
      "history books, perhaps a recent one you might remember is \"The Search\n",
      "for Alexander\".  He has also written or edited several books on\n",
      "gardening.\n",
      "--\n",
      "=Jim  eggertj@ll.mit.edu (Jim Eggert)\n",
      "\n",
      "5\n",
      "soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "num = 2\n",
    "unseen_document = newsgroups_test.data[num]\n",
    "print(unseen_document)\n",
    "print(newsgroups_test.target[num])\n",
    "print(newsgroups_test.target_names[newsgroups_test.target[num]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.6471687), (3, 0.19757356), (1, 0.14305289)]\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing step for the unseen document\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "pred = sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicts soc.religion.christian with a probability of 64.72%\n"
     ]
    }
   ],
   "source": [
    "print('predicts {} with a probability of {:.2f}%'.format(categories_map[pred[0][0]], pred[0][1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model correctly classifies the unseen document with 'x'% probability to the X category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "pool = multiprocessing.Pool()\n",
    "test_processed_docs = list(pool.map(preprocess, newsgroups_test.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bow_corpus = [dictionary.doc2bow(doc) for doc in test_processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comp.windows.x',\n",
       " 'rec.autos',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_test.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i, doc in enumerate(test_bow_corpus):\n",
    "    pred_all = sorted(lda_model[doc], key=lambda tup: -1*tup[1])\n",
    "    pred_cat = categories_map[pred_all[0][0]]\n",
    "    y_pred.append(newsgroups_test.target_names.index(pred_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is the proportion of correct predictions of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6985052861830113"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a confusion matrix \n",
    "from sklearn.metrics import confusion_matrix \n",
    "cm = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y_pred ->, y_true \\/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[362,  11,   0,   8,  12,   2,   0],\n",
       "       [  5, 317,   0,  33,  11,  27,   3],\n",
       "       [  6,   8,   0, 356,   5,  18,   4],\n",
       "       [  0,   7,   0, 382,   2,   6,   2],\n",
       "       [  5,  13,   0,   3, 325,  25,  23],\n",
       "       [  3,   3,   0,   3,   5, 381,   3],\n",
       "       [  2, 174,   0,   5,   2,  32, 149]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyLDAvis.gensim\n",
    "# pyLDAvis.enable_notebook()\n",
    "# prepared = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)\n",
    "# pyLDAvis.show(prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
